services:
  # =============================================================================
  # GPT-OSS 20B - General Purpose Chat (Port 11435)
  # =============================================================================
  gpt-oss-20b:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: gpt-oss-20b
    ports:
      - "11435:8080"
    volumes:
      - /data/ollama:/data/ollama:ro
      - ./models:/models
    environment:
      MODEL_PATH: gpt-oss-20b-Q4_K_M.gguf
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    command: >
      -m /models/${MODEL_PATH:-gpt-oss-20b-Q4_K_M.gguf}
      --host 0.0.0.0
      --port 8080
      -c 16384
      -ngl 99
      --temp 0
      --flash-attn on
      --top-k 40
      --top-p 0.95
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # =============================================================================
  # Ministral-3 14B - Vision + 256K Context Chat (Port 11436)
  # =============================================================================
  ministral-14b:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: ministral-14b
    ports:
      - "11436:8080"
    volumes:
      - ./models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    command: >
      -m /models/Ministral-3-14B-Instruct-2512-Q4_K_M.gguf
      --host 0.0.0.0
      --port 8080
      -c 8192
      -ngl 99
      --temp 0.7
      --flash-attn on
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # =============================================================================
  # Embedding Gemma 300M - Embeddings Only (Port 11437)
  # =============================================================================
  embeddinggemma:
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: embeddinggemma
    ports:
      - "11437:8080"
    volumes:
      - ./models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    command: >
      -m /models/embeddinggemma-300M-Q8_0.gguf
      --host 0.0.0.0
      --port 8080
      --embeddings
      -c 2048
      -ngl 99
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
